{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_lightning-2.4.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/torchmetrics-1.5.2-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_tabnet-4.1.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/einops-0.7.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_tabular-1.1.1-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:35.153117Z",
     "iopub.status.busy": "2025-03-05T10:14:35.152853Z",
     "iopub.status.idle": "2025-03-05T10:14:37.326361Z",
     "shell.execute_reply": "2025-03-05T10:14:37.325725Z",
     "shell.execute_reply.started": "2025-03-05T10:14:35.153097Z"
    },
    "id": "FQN_WtoA1W4l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, quantile_transform\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import NelsonAalenFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:37.327629Z",
     "iopub.status.busy": "2025-03-05T10:14:37.327173Z",
     "iopub.status.idle": "2025-03-05T10:14:37.334225Z",
     "shell.execute_reply": "2025-03-05T10:14:37.333193Z",
     "shell.execute_reply.started": "2025-03-05T10:14:37.327598Z"
    },
    "id": "fxa1I8yL1Zbr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = \"id\"\n",
    "    >>> y_pred = {'prediction': {0: 1.0, 1: 0.0, 2: 1.0}}\n",
    "    >>> y_pred = pd.DataFrame(y_pred)\n",
    "    >>> y_pred.insert(0, row_id_column_name, range(len(y_pred)))\n",
    "    >>> y_true = { 'efs': {0: 1.0, 1: 0.0, 2: 0.0}, 'efs_time': {0: 25.1234,1: 250.1234,2: 2500.1234}, 'race_group': {0: 'race_group_1', 1: 'race_group_1', 2: 'race_group_1'}}\n",
    "    >>> y_true = pd.DataFrame(y_true)\n",
    "    >>> y_true.insert(0, row_id_column_name, range(len(y_true)))\n",
    "    >>> score(y_true.copy(), y_pred.copy(), row_id_column_name)\n",
    "    0.75\n",
    "    \"\"\"\n",
    "\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "\n",
    "    event_label = 'efs'\n",
    "    interval_label = 'efs_time'\n",
    "    prediction_label = 'prediction'\n",
    "    for col in submission.columns:\n",
    "        if not pandas.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "    # Merging solution and submission dfs on ID\n",
    "    merged_df = pd.concat([solution, submission], axis=1)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n",
    "    metric_list = []\n",
    "    for race in merged_df_race_dict.keys():\n",
    "        # Retrieving values from y_test based on index\n",
    "        indices = sorted(merged_df_race_dict[race])\n",
    "        merged_df_race = merged_df.iloc[indices]\n",
    "        # Calculate the concordance index\n",
    "        c_index_race = concordance_index(\n",
    "                        merged_df_race[interval_label],\n",
    "                        -merged_df_race[prediction_label],\n",
    "                        merged_df_race[event_label])\n",
    "        metric_list.append(c_index_race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Event-Masked PRL-NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "ROOT_DATA_PATH = Path(r\"/kaggle/input/equity-post-HCT-survival-predictions\")\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "train = pd.read_csv(ROOT_DATA_PATH.joinpath(\"train.csv\"))\n",
    "test = pd.read_csv(ROOT_DATA_PATH.joinpath(\"test.csv\"))\n",
    "\n",
    "CATEGORICAL_VARIABLES = [\n",
    "    # Graft and HCT reasons\n",
    "    'dri_score', 'graft_type', 'prod_type', 'prim_disease_hct',\n",
    "\n",
    "    # Patient health status (risk factors)\n",
    "    'psych_disturb', 'diabetes', 'arrhythmia', 'vent_hist', 'renal_issue', 'pulm_moderate',\n",
    "    'pulm_severe', 'obesity', 'hepatic_mild', 'hepatic_severe', 'peptic_ulcer', 'rheum_issue',\n",
    "    'cardiac', 'prior_tumor', 'mrd_hct', 'tbi_status', 'cyto_score', 'cyto_score_detail', \n",
    "\n",
    "    # Patient demographics\n",
    "    'ethnicity', 'race_group',\n",
    "\n",
    "    # Biological matching with donor\n",
    "    'sex_match', 'donor_related', 'cmv_status', 'tce_imm_match', 'tce_match', 'tce_div_match',\n",
    "\n",
    "    # Medication/operation related data\n",
    "    'melphalan_dose', 'rituximab', 'gvhd_proph', 'in_vivo_tcd', 'conditioning_intensity'\n",
    "]\n",
    "\n",
    "HLA_COLUMNS = [\n",
    "    'hla_match_a_low', 'hla_match_a_high',\n",
    "    'hla_match_b_low', 'hla_match_b_high',\n",
    "    'hla_match_c_low', 'hla_match_c_high',\n",
    "    'hla_match_dqb1_low', 'hla_match_dqb1_high',\n",
    "    'hla_match_drb1_low', 'hla_match_drb1_high',\n",
    "    \n",
    "    # Matching at HLA-A(low), -B(low), -DRB1(high)\n",
    "    'hla_nmdp_6',\n",
    "    # Matching at HLA-A,-B,-DRB1 (low or high)\n",
    "    'hla_low_res_6', 'hla_high_res_6',\n",
    "    # Matching at HLA-A, -B, -C, -DRB1 (low or high)\n",
    "    'hla_low_res_8', 'hla_high_res_8',\n",
    "    # Matching at HLA-A, -B, -C, -DRB1, -DQB1 (low or high)\n",
    "    'hla_low_res_10', 'hla_high_res_10'\n",
    "]\n",
    "\n",
    "OTHER_NUMERICAL_VARIABLES = ['year_hct', 'donor_age', 'age_at_hct', 'comorbidity_score', 'karnofsky_score']\n",
    "NUMERICAL_VARIABLES = HLA_COLUMNS + OTHER_NUMERICAL_VARIABLES\n",
    "\n",
    "TARGET_VARIABLES = ['efs_time', 'efs']\n",
    "ID_COLUMN = [\"ID\"]\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df[CATEGORICAL_VARIABLES] = df[CATEGORICAL_VARIABLES].fillna(\"Unknown\")\n",
    "    df[OTHER_NUMERICAL_VARIABLES] = df[OTHER_NUMERICAL_VARIABLES].fillna(df[OTHER_NUMERICAL_VARIABLES].median())\n",
    "\n",
    "    return df\n",
    "\n",
    "train = preprocess_data(train)\n",
    "test = preprocess_data(test)\n",
    "\n",
    "\n",
    "def features_engineering(df):\n",
    "    # Change year_hct to relative year from 2000\n",
    "    df['year_hct'] = df['year_hct'] - 2000\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train = features_engineering(train)\n",
    "test = features_engineering(test)\n",
    "\n",
    "train[CATEGORICAL_VARIABLES] = train[CATEGORICAL_VARIABLES].astype('category')\n",
    "test[CATEGORICAL_VARIABLES] = test[CATEGORICAL_VARIABLES].astype('category')\n",
    "\n",
    "FEATURES = train.drop(columns=['ID', 'efs', 'efs_time']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "FOLDS = 5\n",
    "kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_xgb = np.zeros(len(train))\n",
    "pred_efs = np.zeros(len(test))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train, train[\"efs\"])):\n",
    "\n",
    "    print(\"#\"*25)\n",
    "    print(f\"### Fold {i+1}\")\n",
    "    print(\"#\"*25)\n",
    "    \n",
    "    x_train = train.loc[train_index, FEATURES].copy()\n",
    "    y_train = train.loc[train_index, \"efs\"]\n",
    "    x_valid = train.loc[test_index, FEATURES].copy()\n",
    "    y_valid = train.loc[test_index, \"efs\"]\n",
    "    x_test = test[FEATURES].copy()\n",
    "\n",
    "    model_xgb = XGBClassifier(\n",
    "        device=\"cuda\",\n",
    "        max_depth=3,  \n",
    "        colsample_bytree=0.7129400756425178, \n",
    "        subsample=0.8185881823156917, \n",
    "        n_estimators=20_000, \n",
    "        learning_rate=0.04425768131771064,  \n",
    "        eval_metric=\"auc\", \n",
    "        early_stopping_rounds=50, \n",
    "        objective='binary:logistic',\n",
    "        scale_pos_weight=1.5379160847615545,  \n",
    "        min_child_weight=4,\n",
    "        enable_categorical=True,\n",
    "        gamma=3.1330719334577584\n",
    "    )\n",
    "    model_xgb.fit(\n",
    "        x_train, y_train,\n",
    "        eval_set=[(x_valid, y_valid)],  \n",
    "        verbose=100\n",
    "    )\n",
    "\n",
    "    # INFER OOF (Probabilities -> Binary)\n",
    "    oof_xgb[test_index] = (model_xgb.predict_proba(x_valid)[:, 1] > 0.5).astype(int)\n",
    "    # INFER TEST (Probabilities -> Average Probs)\n",
    "    pred_efs += model_xgb.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# COMPUTE AVERAGE TEST PREDS\n",
    "pred_efs = (pred_efs / FOLDS > 0.5).astype(int)\n",
    "\n",
    "# EVALUATE PERFORMANCE\n",
    "accuracy = accuracy_score(train[\"efs\"], oof_xgb)\n",
    "f1 = f1_score(train[\"efs\"], oof_xgb)\n",
    "roc_auc = roc_auc_score(train[\"efs\"], oof_xgb)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def get_X_cat(df, cat_cols, transformers=None):\n",
    "    \"\"\"\n",
    "    Apply a specific categorical data transformer or a LabelEncoder if None.\n",
    "    \"\"\"\n",
    "    if transformers is None:\n",
    "        transformers = [LabelEncoder().fit(df[col]) for col in cat_cols]\n",
    "    return transformers, np.array(\n",
    "        [transformer.transform(df[col]) for col, transformer in zip(cat_cols, transformers)]\n",
    "    ).T\n",
    "\n",
    "\n",
    "def preprocess_data(train, val):\n",
    "    \"\"\"\n",
    "    Standardize numerical variables and transform (Label-encode) categoricals.\n",
    "    Fill NA values with mean for numerical.\n",
    "    Create torch dataloaders to prepare data for training and evaluation.\n",
    "    \"\"\"\n",
    "    X_cat_train, X_cat_val, numerical, transformers = get_categoricals(train, val)\n",
    "    scaler = StandardScaler()\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n",
    "    X_num_train = imp.fit_transform(train[numerical])\n",
    "    X_num_train = scaler.fit_transform(X_num_train)\n",
    "    X_num_val = imp.transform(val[numerical])\n",
    "    X_num_val = scaler.transform(X_num_val)\n",
    "    dl_train = init_dl(X_cat_train, X_num_train, train, training=True)\n",
    "    dl_val = init_dl(X_cat_val, X_num_val, val)\n",
    "    return X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers\n",
    "\n",
    "\n",
    "def get_categoricals(train, val):\n",
    "    \"\"\"\n",
    "    Remove constant categorical columns and transform them using LabelEncoder.\n",
    "    Return the label-transformers for each categorical column, categorical dataframes and numerical columns.\n",
    "    \"\"\"\n",
    "    categorical_cols, numerical = get_feature_types(train)\n",
    "    remove = []\n",
    "    for col in categorical_cols:\n",
    "        if train[col].nunique() == 1:\n",
    "            remove.append(col)\n",
    "        ind = ~val[col].isin(train[col])\n",
    "        if ind.any():\n",
    "            val.loc[ind, col] = np.nan\n",
    "    categorical_cols = [col for col in categorical_cols if col not in remove]\n",
    "    transformers, X_cat_train = get_X_cat(train, categorical_cols)\n",
    "    _, X_cat_val = get_X_cat(val, categorical_cols, transformers)\n",
    "    return X_cat_train, X_cat_val, numerical, transformers\n",
    "\n",
    "\n",
    "def init_dl(X_cat, X_num, df, training=False):\n",
    "    \"\"\"\n",
    "    Initialize data loaders with 4 dimensions : categorical dataframe, numerical dataframe and target values (efs and efs_time).\n",
    "    Notice that efs_time is log-transformed.\n",
    "    Fix batch size to 2048 and return dataloader for training or validation depending on training value.\n",
    "    \"\"\"\n",
    "    ds_train = TensorDataset(\n",
    "        torch.tensor(X_cat, dtype=torch.long),\n",
    "        torch.tensor(X_num, dtype=torch.float32),\n",
    "        torch.tensor(df.efs_time.values, dtype=torch.float32).log(),\n",
    "        torch.tensor(df.efs.values, dtype=torch.long)\n",
    "    )\n",
    "    bs = 2048\n",
    "    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n",
    "    return dl_train\n",
    "\n",
    "\n",
    "def get_feature_types(train):\n",
    "    \"\"\"\n",
    "    Utility function to return categorical and numerical column names.\n",
    "    \"\"\"\n",
    "    categorical_cols = [col for i, col in enumerate(train.columns) if ((train[col].dtype == \"object\") | (2 < train[col].nunique() < 25))]\n",
    "    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n",
    "    FEATURES = [c for c in train.columns if not c in RMV]\n",
    "    numerical = [i for i in FEATURES if i not in categorical_cols]\n",
    "    return categorical_cols, numerical\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    \"\"\"\n",
    "    Create some new features to help the model focus on specific patterns.\n",
    "    \"\"\"\n",
    "    # sex_match = df.sex_match.astype(str)\n",
    "    # sex_match = sex_match.str.split(\"-\").str[0] == sex_match.str.split(\"-\").str[1]\n",
    "    # df['sex_match_bool'] = sex_match\n",
    "    # df.loc[df.sex_match.isna(), 'sex_match_bool'] = np.nan\n",
    "    # df['big_age'] = df.age_at_hct > 16\n",
    "    # df.loc[df.year_hct == 2019, 'year_hct'] = 2020\n",
    "    df['is_cyto_score_same'] = (df['cyto_score'] == df['cyto_score_detail']).astype(int)\n",
    "    # df['strange_age'] = df.age_at_hct == 0.044\n",
    "    # df['age_bin'] = pd.cut(df.age_at_hct, [0, 0.0441, 16, 30, 50, 100])\n",
    "    # df['age_ts'] = df.age_at_hct / df.donor_age\n",
    "    df['year_hct'] -= 2000\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load data and add features.\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "    test = add_features(test)\n",
    "    print(\"Test shape:\", test.shape)\n",
    "    train = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n",
    "    train = add_features(train)\n",
    "    print(\"Train shape:\", train.shape)\n",
    "    return test, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import List\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "from lifelines.utils import concordance_index\n",
    "from pytorch_lightning.cli import ReduceLROnPlateau\n",
    "from pytorch_tabular.models.common.layers import ODST\n",
    "from torch import nn\n",
    "from pytorch_lightning.utilities import grad_norm\n",
    "\n",
    "\n",
    "class CatEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding module for the categorical dataframe.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        projection_dim: int,\n",
    "        categorical_cardinality: List[int],\n",
    "        embedding_dim: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        projection_dim: The dimension of the final output after projecting the concatenated embeddings into a lower-dimensional space.\n",
    "        categorical_cardinality: A list where each element represents the number of unique categories (cardinality) in each categorical feature.\n",
    "        embedding_dim: The size of the embedding space for each categorical feature.\n",
    "        self.embeddings: list of embedding layers for each categorical feature.\n",
    "        self.projection: sequential neural network that goes from the embedding to the output projection dimension with GELU activation.\n",
    "        \"\"\"\n",
    "        super(CatEmbeddings, self).__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cardinality, embedding_dim)\n",
    "            for cardinality in categorical_cardinality\n",
    "        ])\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * len(categorical_cardinality), projection_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(projection_dim, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat):\n",
    "        \"\"\"\n",
    "        Apply the projection on concatened embeddings that contains all categorical features.\n",
    "        \"\"\"\n",
    "        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]\n",
    "        x_cat = torch.cat(x_cat, dim=1)\n",
    "        return self.projection(x_cat)\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    \"\"\"\n",
    "    Train a model on both categorical embeddings and numerical data.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            continuous_dim: int,\n",
    "            categorical_cardinality: List[int],\n",
    "            embedding_dim: int,\n",
    "            projection_dim: int,\n",
    "            hidden_dim: int,\n",
    "            dropout: float = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        continuous_dim: The number of continuous features.\n",
    "        categorical_cardinality: A list of integers representing the number of unique categories in each categorical feature.\n",
    "        embedding_dim: The dimensionality of the embedding space for each categorical feature.\n",
    "        projection_dim: The size of the projected output space for the categorical embeddings.\n",
    "        hidden_dim: The number of neurons in the hidden layer of the MLP.\n",
    "        dropout: The dropout rate applied in the network.\n",
    "        self.embeddings: previous embeddings for categorical data.\n",
    "        self.mlp: defines an MLP model with an ODST layer followed by batch normalization and dropout.\n",
    "        self.out: linear output layer that maps the output of the MLP to a single value\n",
    "        self.dropout: defines dropout\n",
    "        Weights initialization with xavier normal algorithm and biases with zeros.\n",
    "        \"\"\"\n",
    "        super(NN, self).__init__()\n",
    "        self.embeddings = CatEmbeddings(projection_dim, categorical_cardinality, embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            ODST(projection_dim + continuous_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        \"\"\"\n",
    "        Create embedding layers for categorical data, concatenate with continous variables.\n",
    "        Add dropout and goes through MLP and return raw output and 1-dimensional output as well.\n",
    "        \"\"\"\n",
    "        x = self.embeddings(x_cat)\n",
    "        x = torch.cat([x, x_cont], dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x), x\n",
    "\n",
    "\n",
    "@functools.lru_cache\n",
    "def combinations(N):\n",
    "    \"\"\"\n",
    "    calculates all possible 2-combinations (pairs) of a tensor of indices from 0 to N-1, \n",
    "    and caches the result using functools.lru_cache for optimization\n",
    "    \"\"\"\n",
    "    ind = torch.arange(N)\n",
    "    comb = torch.combinations(ind, r=2)\n",
    "    return comb.cuda()\n",
    "\n",
    "\n",
    "class LitNN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Main Model creation and losses definition to fully train the model.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            continuous_dim: int,\n",
    "            categorical_cardinality: List[int],\n",
    "            embedding_dim: int,\n",
    "            projection_dim: int,\n",
    "            hidden_dim: int,\n",
    "            lr: float = 1e-3,\n",
    "            dropout: float = 0.2,\n",
    "            weight_decay: float = 1e-3,\n",
    "            aux_weight: float = 0.1,\n",
    "            margin: float = 0.5,\n",
    "            race_index: int = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        continuous_dim: The number of continuous input features.\n",
    "        categorical_cardinality: A list of integers, where each element corresponds to the number of unique categories for each categorical feature.\n",
    "        embedding_dim: The dimension of the embeddings for the categorical features.\n",
    "        projection_dim: The dimension of the projected space after embedding concatenation.\n",
    "        hidden_dim: The size of the hidden layers in the feedforward network (MLP).\n",
    "        lr: The learning rate for the optimizer.\n",
    "        dropout: Dropout probability to avoid overfitting.\n",
    "        weight_decay: The L2 regularization term for the optimizer.\n",
    "        aux_weight: Weight used for auxiliary tasks.\n",
    "        margin: Margin used in some loss functions.\n",
    "        race_index: An index that refer to race_group in the input data.\n",
    "        \"\"\"\n",
    "        super(LitNN, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Creates an instance of the NN model defined above\n",
    "        self.model = NN(\n",
    "            continuous_dim=self.hparams.continuous_dim,\n",
    "            categorical_cardinality=self.hparams.categorical_cardinality,\n",
    "            embedding_dim=self.hparams.embedding_dim,\n",
    "            projection_dim=self.hparams.projection_dim,\n",
    "            hidden_dim=self.hparams.hidden_dim,\n",
    "            dropout=self.hparams.dropout\n",
    "        )\n",
    "        self.targets = []\n",
    "\n",
    "        # Defines a small feedforward neural network that performs an auxiliary task with 1-dimensional output\n",
    "        self.aux_cls = nn.Sequential(\n",
    "            nn.Linear(self.hparams.hidden_dim, self.hparams.hidden_dim // 3),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hparams.hidden_dim // 3, 1)\n",
    "        )\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "        \"\"\"\n",
    "        Compute the 2-norm for each layer\n",
    "        If using mixed precision, the gradients are already unscaled here\n",
    "        \"\"\"\n",
    "        norms = grad_norm(self.model, norm_type=2)\n",
    "        self.log_dict(norms)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        \"\"\"\n",
    "        Forward pass that outputs the 1-dimensional prediction and the embeddings (raw output)\n",
    "        \"\"\"\n",
    "        x, emb = self.model(x_cat, x_cont)\n",
    "        return x.squeeze(1), emb\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        defines how the model processes each batch of data during training.\n",
    "        A batch is a combination of : categorical data, continuous data, efs_time (y) and efs event.\n",
    "        y_hat is the efs_time prediction on all data and aux_pred is auxiliary prediction on embeddings.\n",
    "        Calculates loss and race_group loss on full data.\n",
    "        Auxiliary loss is calculated with an event mask, ignoring efs=0 predictions and taking the average.\n",
    "        Returns loss and aux_loss multiplied by weight defined above.\n",
    "        \"\"\"\n",
    "        x_cat, x_cont, y, efs = batch\n",
    "        y_hat, emb = self(x_cat, x_cont)\n",
    "        aux_pred = self.aux_cls(emb).squeeze(1)\n",
    "        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n",
    "        aux_loss = nn.functional.mse_loss(aux_pred, y, reduction='none')\n",
    "        aux_mask = efs == 1\n",
    "        aux_loss = (aux_loss * aux_mask).sum() / aux_mask.sum()\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"race_loss\", race_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        self.log(\"aux_loss\", aux_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        return loss + aux_loss * self.hparams.aux_weight\n",
    "\n",
    "    def get_full_loss(self, efs, x_cat, y, y_hat):\n",
    "        \"\"\"\n",
    "        Output loss and race_group loss.\n",
    "        \"\"\"\n",
    "        loss = self.calc_loss(y, y_hat, efs)\n",
    "        race_loss = self.get_race_losses(efs, x_cat, y, y_hat)\n",
    "        loss += 0.1 * race_loss\n",
    "        return loss, race_loss\n",
    "\n",
    "    def get_race_losses(self, efs, x_cat, y, y_hat):\n",
    "        \"\"\"\n",
    "        Calculate loss for each race_group based on deviation/variance.\n",
    "        \"\"\"\n",
    "        races = torch.unique(x_cat[:, self.hparams.race_index])\n",
    "        race_losses = []\n",
    "        for race in races:\n",
    "            ind = x_cat[:, self.hparams.race_index] == race\n",
    "            race_losses.append(self.calc_loss(y[ind], y_hat[ind], efs[ind]))\n",
    "        race_loss = sum(race_losses) / len(race_losses)\n",
    "        races_loss_std = sum((r - race_loss)**2 for r in race_losses) / len(race_losses)\n",
    "        return torch.sqrt(races_loss_std)\n",
    "\n",
    "    def calc_loss(self, y, y_hat, efs):\n",
    "        \"\"\"\n",
    "        Most important part of the model : loss function used for training.\n",
    "        We face survival data with event indicators along with time-to-event.\n",
    "\n",
    "        This function computes the main loss by the following the steps :\n",
    "        * create all data pairs with \"combinations\" function (= all \"two subjects\" combinations)\n",
    "        * make sure that we have at least 1 event in each pair\n",
    "        * convert y to +1 or -1 depending on the correct ranking\n",
    "        * loss is computed using a margin-based hinge loss\n",
    "        * mask is applied to ensure only valid pairs are being used (censored data can't be ranked with event in some cases)\n",
    "        * average loss on all pairs is returned\n",
    "        \"\"\"\n",
    "        N = y.shape[0]\n",
    "        comb = combinations(N)\n",
    "        comb = comb[(efs[comb[:, 0]] == 1) | (efs[comb[:, 1]] == 1)]\n",
    "        pred_left = y_hat[comb[:, 0]]\n",
    "        pred_right = y_hat[comb[:, 1]]\n",
    "        y_left = y[comb[:, 0]]\n",
    "        y_right = y[comb[:, 1]]\n",
    "        y = 2 * (y_left > y_right).int() - 1\n",
    "        loss = nn.functional.relu(-y * (pred_left - pred_right) + self.hparams.margin)\n",
    "        mask = self.get_mask(comb, efs, y_left, y_right)\n",
    "        loss = (loss.double() * (mask.double())).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "    def get_mask(self, comb, efs, y_left, y_right):\n",
    "        \"\"\"\n",
    "        Defines all invalid comparisons :\n",
    "        * Case 1: \"Left outlived Right\" but Right is censored\n",
    "        * Case 2: \"Right outlived Left\" but Left is censored\n",
    "        Masks for case 1 and case 2 are combined using |= operator and inverted using ~ to create a \"valid pair mask\"\n",
    "        \"\"\"\n",
    "        left_outlived = y_left >= y_right\n",
    "        left_1_right_0 = (efs[comb[:, 0]] == 1) & (efs[comb[:, 1]] == 0)\n",
    "        mask2 = (left_outlived & left_1_right_0)\n",
    "        right_outlived = y_right >= y_left\n",
    "        right_1_left_0 = (efs[comb[:, 1]] == 1) & (efs[comb[:, 0]] == 0)\n",
    "        mask2 |= (right_outlived & right_1_left_0)\n",
    "        mask2 = ~mask2\n",
    "        mask = mask2\n",
    "        return mask\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        This method defines how the model processes each batch during validation\n",
    "        \"\"\"\n",
    "        x_cat, x_cont, y, efs = batch\n",
    "        y_hat, emb = self(x_cat, x_cont)\n",
    "        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n",
    "        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        At the end of the validation epoch, it computes and logs the concordance index\n",
    "        \"\"\"\n",
    "        cindex, metric = self._calc_cindex()\n",
    "        self.log(\"cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.targets.clear()\n",
    "\n",
    "    def _calc_cindex(self):\n",
    "        \"\"\"\n",
    "        Calculate c-index accounting for each race_group or global.\n",
    "        \"\"\"\n",
    "        y = torch.cat([t[0] for t in self.targets]).cpu().numpy()\n",
    "        y_hat = torch.cat([t[1] for t in self.targets]).cpu().numpy()\n",
    "        efs = torch.cat([t[2] for t in self.targets]).cpu().numpy()\n",
    "        races = torch.cat([t[3] for t in self.targets]).cpu().numpy()\n",
    "        metric = self._metric(efs, races, y, y_hat)\n",
    "        cindex = concordance_index(y, y_hat, efs)\n",
    "        return cindex, metric\n",
    "\n",
    "    def _metric(self, efs, races, y, y_hat):\n",
    "        \"\"\"\n",
    "        Calculate c-index accounting for each race_group\n",
    "        \"\"\"\n",
    "        metric_list = []\n",
    "        for race in np.unique(races):\n",
    "            y_ = y[races == race]\n",
    "            y_hat_ = y_hat[races == race]\n",
    "            efs_ = efs[races == race]\n",
    "            metric_list.append(concordance_index(y_, y_hat_, efs_))\n",
    "        metric = float(np.mean(metric_list) - np.sqrt(np.var(metric_list)))\n",
    "        return metric\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Same as training step but to log test data\n",
    "        \"\"\"\n",
    "        x_cat, x_cont, y, efs = batch\n",
    "        y_hat, emb = self(x_cat, x_cont)\n",
    "        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n",
    "        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        At the end of the test epoch, calculates and logs the concordance index for the test set\n",
    "        \"\"\"\n",
    "        cindex, metric = self._calc_cindex()\n",
    "        self.log(\"test_cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.targets.clear()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        configures the optimizer and learning rate scheduler:\n",
    "        * Optimizer: Adam optimizer with weight decay (L2 regularization).\n",
    "        * Scheduler: Cosine Annealing scheduler, which adjusts the learning rate according to a cosine curve.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        scheduler_config = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=45,\n",
    "                eta_min=6e-3\n",
    "            ),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"strict\": False,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import StochasticWeightAveraging\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "def main(hparams):\n",
    "    \"\"\"\n",
    "    Main function to train the model.\n",
    "    The steps are as following :\n",
    "    * load data and fill efs and efs time for test data with 1\n",
    "    * initialize pred array with 0\n",
    "    * get categorical and numerical columns\n",
    "    * split the train data on the stratified criterion : race_group * newborns yes/no\n",
    "    * preprocess the fold data (create dataloaders)\n",
    "    * train the model and create final submission output\n",
    "    \"\"\"\n",
    "    test, train_original = load_data()\n",
    "    test['efs_time'] = 1\n",
    "    test['efs'] = 1\n",
    "    oof_nn_pairwise = np.zeros(len(train_original))\n",
    "    test_pred = np.zeros(test.shape[0])\n",
    "    categorical_cols, numerical = get_feature_types(train_original)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, )\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        kf.split(\n",
    "            train_original, train_original.race_group.astype(str) + (train_original.age_at_hct == 0.044).astype(str)\n",
    "        )\n",
    "    ):\n",
    "        tt = train_original.copy()\n",
    "        train = tt.iloc[train_index]\n",
    "        val = tt.iloc[test_index]\n",
    "        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, val)\n",
    "        model = train_final(X_num_train, dl_train, dl_val, transformers, categorical_cols=categorical_cols)\n",
    "        oof_pred, _ = model.cuda().eval()(\n",
    "            torch.tensor(X_cat_val, dtype=torch.long).cuda(),\n",
    "            torch.tensor(X_num_val, dtype=torch.float32).cuda()\n",
    "        )\n",
    "        oof_nn_pairwise[test_index] = oof_pred.detach().cpu().numpy()\n",
    "        # Create submission\n",
    "        train = tt.iloc[train_index]\n",
    "        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, test)\n",
    "        pred, _ = model.cuda().eval()(\n",
    "            torch.tensor(X_cat_val, dtype=torch.long).cuda(),\n",
    "            torch.tensor(X_num_val, dtype=torch.float32).cuda()\n",
    "        )\n",
    "        test_pred += pred.detach().cpu().numpy()\n",
    "        \n",
    "    \n",
    "    return -test_pred, -oof_nn_pairwise\n",
    "\n",
    "\n",
    "\n",
    "def train_final(X_num_train, dl_train, dl_val, transformers, hparams=None, categorical_cols=None):\n",
    "    \"\"\"\n",
    "    Defines model hyperparameters and fit the model.\n",
    "    \"\"\"\n",
    "    if hparams is None:\n",
    "        hparams = {\n",
    "            \"embedding_dim\": 16,\n",
    "            \"projection_dim\": 112,\n",
    "            \"hidden_dim\": 56,\n",
    "            \"lr\": 0.06464861983337984,\n",
    "            \"dropout\": 0.05463240181423116,\n",
    "            \"aux_weight\": 0.26545778308743806,\n",
    "            \"margin\": 0.2588153271003354,\n",
    "            \"weight_decay\": 0.0002773544957610778\n",
    "        }\n",
    "    model = LitNN(\n",
    "        continuous_dim=X_num_train.shape[1],\n",
    "        categorical_cardinality=[len(t.classes_) for t in transformers],\n",
    "        race_index=categorical_cols.index(\"race_group\"),\n",
    "        **hparams\n",
    "    )\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_top_k=1)\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='cuda',\n",
    "        max_epochs=60,\n",
    "        log_every_n_steps=6,\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            LearningRateMonitor(logging_interval='epoch'),\n",
    "            TQDMProgressBar(),\n",
    "            StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=45, annealing_epochs=15)\n",
    "        ],\n",
    "    )\n",
    "    trainer.fit(model, dl_train)\n",
    "    trainer.test(model, dl_val)\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hparams = None\n",
    "pairwise_ranking_pred, pairwise_ranking_oof = main(hparams)\n",
    "\n",
    "y_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "y_pred = train[[\"ID\"]].copy()\n",
    "y_pred[\"prediction\"] = pairwise_ranking_oof\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(f\"\\nPairwise ranking NN CV =\",m)\n",
    "\n",
    "# Update predictions with classifier mask\n",
    "pairwise_ranking_oof[oof_xgb == 1] += 0.2\n",
    "y_pred[\"prediction\"] = pairwise_ranking_oof\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(f\"\\nPairwise ranking NN with classifier mask -> CV =\",m)\n",
    "\n",
    "pairwise_ranking_pred[pred_efs == 1] += 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "subm_data = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n",
    "subm_data['prediction'] = pairwise_ranking_pred\n",
    "subm_data.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Yunbase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --requirement /kaggle/input/yunbase/Yunbase/requirements.txt  \\\n",
    "--no-index --find-links file:/kaggle/input/yunbase/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "source_file_path = '/kaggle/input/yunbase/Yunbase/baseline.py'\n",
    "target_file_path = '/kaggle/working/baseline.py'\n",
    "with open(source_file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "with open(target_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from baseline import Yunbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random#provide some function to generate random_seed.\n",
    "#set random seed,to make sure model can be recurrented.\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)#numpy's random seed\n",
    "    random.seed(seed)#python built-in random seed\n",
    "seed_everything(seed=2025)\n",
    "\n",
    "train=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n",
    "test=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "train_solution=train[['ID','efs','efs_time','race_group']].copy()\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p) - np.log(1 - p)\n",
    "max_efs_time,min_efs_time=80,-100\n",
    "train['efs_time']=train['efs_time']/(max_efs_time-min_efs_time)\n",
    "train['efs_time']=train['efs_time'].apply(lambda x:logit(x))\n",
    "train['efs_time']+=10\n",
    "print(train['efs_time'].max(),train['efs_time'].min())\n",
    "\n",
    "race2weight={'American Indian or Alaska Native':0.68,\n",
    "'Asian':0.7,'Black or African-American':0.67,\n",
    "'More than one race':0.68,\n",
    "'Native Hawaiian or other Pacific Islander':0.66,\n",
    "'White':0.64}\n",
    "train['weight']=0.5*train['efs']+0.5\n",
    "train['raceweight']=train['race_group'].apply(lambda x:race2weight.get(x,1))\n",
    "train['weight']=train['weight']/train['raceweight']\n",
    "train.drop(['raceweight'],axis=1,inplace=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    \n",
    "    kmf.fit(df[time_col], event_observed=df[event_col])\n",
    "    \n",
    "    survival_probabilities = kmf.survival_function_at_times(df[time_col]).values.flatten()\n",
    "\n",
    "    return survival_probabilities\n",
    "\n",
    "race_group=sorted(train['race_group'].unique())\n",
    "for race in race_group:\n",
    "    train.loc[train['race_group']==race,\"target\"] = transform_survival_probability(train[train['race_group']==race], time_col='efs_time', event_col='efs')\n",
    "    gap=0.7*(train.loc[(train['race_group']==race)&(train['efs']==0)]['target'].max()-train.loc[(train['race_group']==race)&(train['efs']==1)]['target'].min())/2\n",
    "    train.loc[(train['race_group']==race)&(train['efs']==0),'target']-=gap\n",
    "\n",
    "sns.histplot(data=train, x='target', hue='efs', element='step', stat='density', common_norm=False)\n",
    "plt.legend(title='efs')\n",
    "plt.title('Distribution of Target by EFS')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "train.drop(['efs','efs_time'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#nunique=2\n",
    "nunique2=[col for col in train.columns if train[col].nunique()==2 and col!='efs']\n",
    "#nunique<50\n",
    "nunique50=[col for col in train.columns if train[col].nunique()<50 and col not in ['efs','weight']]+['age_group','dri_score_NA']\n",
    "\n",
    "def FE(df):\n",
    "    print(\"< deal with outlier >\")\n",
    "    df['nan_value_each_row'] = df.isnull().sum(axis=1)\n",
    "    #year_hct=2020 only 4 rows.\n",
    "    df['year_hct']=df['year_hct'].replace(2020,2019)\n",
    "    df['age_group']=df['age_at_hct']//10\n",
    "    #karnofsky_score 40 only 10 rows.\n",
    "    df['karnofsky_score']=df['karnofsky_score'].replace(40,50)\n",
    "    #hla_high_res_8=2 only 2 rows.\n",
    "    df['hla_high_res_8']=df['hla_high_res_8'].replace(2,3)\n",
    "    #hla_high_res_6=0 only 1 row.\n",
    "    df['hla_high_res_6']=df['hla_high_res_6'].replace(0,2)\n",
    "    #hla_high_res_10=3 only 1 row.\n",
    "    df['hla_high_res_10']=df['hla_high_res_10'].replace(3,4)\n",
    "    #hla_low_res_8=2 only 1 row.\n",
    "    df['hla_low_res_8']=df['hla_low_res_8'].replace(2,3)\n",
    "    df['dri_score']=df['dri_score'].replace('Missing disease status','N/A - disease not classifiable')\n",
    "    df['dri_score_NA']=df['dri_score'].apply(lambda x:int('N/A' in str(x)))\n",
    "    for col in ['diabetes','pulm_moderate','cardiac']:\n",
    "        df.loc[df[col].isna(),col]='Not done'\n",
    "\n",
    "    print(\"< cross feature >\")\n",
    "    df['donor_age-age_at_hct']=df['donor_age']-df['age_at_hct']\n",
    "    df['comorbidity_score+karnofsky_score']=df['comorbidity_score']+df['karnofsky_score']\n",
    "    df['comorbidity_score-karnofsky_score']=df['comorbidity_score']-df['karnofsky_score']\n",
    "    df['comorbidity_score*karnofsky_score']=df['comorbidity_score']*df['karnofsky_score']\n",
    "    df['comorbidity_score/karnofsky_score']=df['comorbidity_score']/df['karnofsky_score']\n",
    "    \n",
    "    print(\"< fillna >\")\n",
    "    df[nunique50]=df[nunique50].astype(str).fillna('NaN')\n",
    "    \n",
    "    print(\"< combine category feature >\")\n",
    "    for i in range(len(nunique2)):\n",
    "        for j in range(i+1,len(nunique2)):\n",
    "            df[nunique2[i]+nunique2[j]]=df[nunique2[i]].astype(str)+df[nunique2[j]].astype(str)\n",
    "    \n",
    "    print(\"< drop useless columns >\")\n",
    "    df.drop(['ID'],axis=1,inplace=True,errors='ignore')\n",
    "    return df\n",
    "\n",
    "combine_category_cols=[]\n",
    "for i in range(len(nunique2)):\n",
    "    for j in range(i+1,len(nunique2)):\n",
    "        combine_category_cols.append(nunique2[i]+nunique2[j])  \n",
    "\n",
    "total_category_feature=nunique50+combine_category_cols\n",
    "\n",
    "target_stat=[]\n",
    "for j in range(len(total_category_feature)):\n",
    "   for col in ['donor_age','age_at_hct','target']:\n",
    "    target_stat.append( (total_category_feature[j],col,['count','mean','max','std','skew']) )\n",
    "\n",
    "num_folds=8\n",
    "\n",
    "lgb_params={\"boosting_type\": \"gbdt\",\"metric\": 'mae',\n",
    "            'random_state': 2025,  \"max_depth\": 9,\"learning_rate\": 0.1,\n",
    "            \"n_estimators\": 768,\"colsample_bytree\": 0.6,\"colsample_bynode\": 0.6,\n",
    "            \"verbose\": -1,\"reg_alpha\": 0.2,\n",
    "            \"reg_lambda\": 5,\"extra_trees\":True,'num_leaves':64,\"max_bin\":255,\n",
    "            'importance_type': 'gain',#better than 'split'\n",
    "            'device':'gpu','gpu_use_dp':True\n",
    "           }\n",
    "\n",
    "yunbase=Yunbase(num_folds=num_folds,\n",
    "                  models=[(LGBMRegressor(**lgb_params),'lgb')\n",
    "                         ],\n",
    "                  FE=FE,\n",
    "                  seed=2025,\n",
    "                  objective='regression',\n",
    "                  metric='mae',\n",
    "                  target_col='target',\n",
    "                  device='gpu',\n",
    "                  one_hot_max=-1,\n",
    "                  early_stop=1000,\n",
    "                  cross_cols=['donor_age','age_at_hct'],\n",
    "                  target_stat=target_stat,\n",
    "                  use_data_augmentation=True,\n",
    "                  use_scaler=True,\n",
    "                  log=250,\n",
    "                  plot_feature_importance=True,\n",
    "                  #print metric score when model training\n",
    "                  use_eval_metric=False,\n",
    ")\n",
    "yunbase.fit(train,category_cols=nunique2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# weights = [0.5,0.5]\n",
    "weights = [1]\n",
    "\n",
    "lgb_prediction=np.load(f\"Yunbase_info/lgb_seed{yunbase.seed}_repeat0_fold{yunbase.num_folds}_{yunbase.target_col}.npy\")\n",
    "lgb_prediction=pd.DataFrame({'ID':train_solution['ID'],'prediction':lgb_prediction})\n",
    "print(f\"lgb_score:{score(train_solution.copy(),lgb_prediction.copy(),row_id_column_name='ID')}\")\n",
    "\n",
    "y_preds=[\n",
    "    lgb_prediction.copy(),\n",
    "]\n",
    "\n",
    "final_prediction=lgb_prediction.copy()\n",
    "final_prediction['prediction']=0\n",
    "for i in range(len(y_preds)):\n",
    "    final_prediction['prediction']+=weights[i]*y_preds[i]['prediction']\n",
    "metric=score(train_solution.copy(),final_prediction.copy(),row_id_column_name='ID')\n",
    "print(f\"final_CV:{metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_preds=yunbase.predict(test,weights=weights)\n",
    "yunbase.target_col='prediction'\n",
    "yunbase.submit(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\",test_preds,\n",
    "               save_name='submission1'\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensemble XGB+LGB with Transformed Targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:37.335323Z",
     "iopub.status.busy": "2025-03-05T10:14:37.335103Z",
     "iopub.status.idle": "2025-03-05T10:14:37.355425Z",
     "shell.execute_reply": "2025-03-05T10:14:37.354748Z",
     "shell.execute_reply.started": "2025-03-05T10:14:37.335304Z"
    },
    "id": "l82lJSS71h4T",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "  train_path = \"//kaggle/input/equity-post-HCT-survival-predictions/train.csv\"\n",
    "  test_path = \"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\"\n",
    "  subm_path = \"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\"\n",
    "\n",
    "  weights = [0.66, 0.65, 0.67, 0.67, 0.67, 0.66, 0.67]\n",
    "\n",
    "  n_splits = 5\n",
    "  penalizer = 0.01\n",
    "  early_stop = 300\n",
    "\n",
    "  cat_params = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'learning_rate': 0.03,\n",
    "        'random_state': 42,\n",
    "        'task_type': 'CPU',\n",
    "        'num_trees': 6000,\n",
    "        'reg_lambda': 8.0,\n",
    "        'depth': 8\n",
    "  }\n",
    "\n",
    "  #cox_loss\n",
    "  cat_cox_params = {\n",
    "        'grow_policy': 'Lossguide',\n",
    "        'min_child_samples': 2,\n",
    "        'loss_function': 'Cox',\n",
    "        'learning_rate': 0.03,\n",
    "        'random_state': 42,\n",
    "        'num_trees': 6000,\n",
    "        'reg_lambda': 8.0,\n",
    "        'num_leaves': 32,\n",
    "        'depth': 8\n",
    "    }\n",
    "\n",
    "\n",
    "  xgb_params = {\n",
    "        'max_depth': 3,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'subsample': 0.8,\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.02,\n",
    "        'enable_categorical': True,\n",
    "        'min_child_weight': 80\n",
    "  }\n",
    "\n",
    "  #cox_loss\n",
    "  xgb_cox_params = {\n",
    "        'max_depth': 3,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'subsample': 0.8,\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.02,\n",
    "        'enable_categorical': True,\n",
    "        'min_child_weight': 80,\n",
    "        'objective': 'survival:cox',\n",
    "        'eval_metric': 'cox-nloglik'\n",
    "  }\n",
    "\n",
    "  lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'min_child_samples': 32,\n",
    "        'num_iterations': 6000,\n",
    "        'learning_rate': 0.03,\n",
    "        'extra_trees': True,\n",
    "        'reg_lambda': 8.0,\n",
    "        'reg_alpha': 0.1,\n",
    "        'num_leaves': 64,\n",
    "        'metric': 'rmse',\n",
    "        'max_depth': 8,\n",
    "        'importance_type': 'gain',\n",
    "        'max_bin': 128,\n",
    "        'verbose': -1,\n",
    "        'seed': 42\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:37.357574Z",
     "iopub.status.busy": "2025-03-05T10:14:37.357374Z",
     "iopub.status.idle": "2025-03-05T10:14:37.614128Z",
     "shell.execute_reply": "2025-03-05T10:14:37.613374Z",
     "shell.execute_reply.started": "2025-03-05T10:14:37.357557Z"
    },
    "id": "YvDSDy4B1m53",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(CFG.train_path)\n",
    "test = pd.read_csv(CFG.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:37.615969Z",
     "iopub.status.busy": "2025-03-05T10:14:37.615630Z",
     "iopub.status.idle": "2025-03-05T10:14:37.626809Z",
     "shell.execute_reply": "2025-03-05T10:14:37.625863Z",
     "shell.execute_reply.started": "2025-03-05T10:14:37.615930Z"
    },
    "id": "2DF5Cpq11o0D",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  def load_data(self, path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "  def check_df(self, df):\n",
    "    print(\"----- SHAPE -----\")\n",
    "    print(df.shape)\n",
    "    print(\"\\n----- DTYPES -----\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\n----- NA -----\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\n----- UNIQUE -----\")\n",
    "    print(df.nunique())\n",
    "\n",
    "  def fill_hla(self, df):\n",
    "    #filling hla_nmdp_6\n",
    "    df[\"hla_nmdp_6\"].fillna(df[[\"hla_match_a_low\", \"hla_match_b_low\", \"hla_match_drb1_high\"]].sum(axis=1), inplace=True)\n",
    "\n",
    "    #filling hla_low_res_6\n",
    "    df[\"hla_low_res_6\"].fillna(df[[\"hla_match_a_low\", \"hla_match_b_low\", \"hla_match_drb1_low\"]].sum(axis=1), inplace=True)\n",
    "\n",
    "    #filling hla_high_res_6\n",
    "    df[\"hla_high_res_6\"].fillna(df[[\"hla_match_a_high\", \"hla_match_b_high\", \"hla_match_drb1_high\"]].sum(axis=1), inplace=True)\n",
    "\n",
    "    #filling hla_low_res_8\n",
    "    df['hla_low_res_8'].fillna(df[[\"hla_match_a_low\", \"hla_match_b_low\", \"hla_match_c_low\", \"hla_match_drb1_low\"]].sum(axis=1), inplace=True)\n",
    "\n",
    "    #filling hla_high_res_8\n",
    "    df[\"hla_high_res_8\"].fillna(df[[\"hla_match_a_high\", \"hla_match_b_high\", \"hla_match_c_high\", \"hla_match_drb1_high\"]].sum(axis=1), inplace=True)\n",
    "\n",
    "    #filling hla_low_res_10\n",
    "    df[\"hla_low_res_10\"].fillna(df[[\"hla_match_a_low\", \"hla_match_b_low\", \"hla_match_c_low\", \"hla_match_drb1_low\", \"hla_match_dqb1_low\"]].sum(axis=1), inplace=True)\n",
    "\n",
    "    #filling hla_high_res_10\n",
    "    df[\"hla_high_res_10\"].fillna(df[[\"hla_match_a_high\", \"hla_match_b_high\", \"hla_match_c_high\", \"hla_match_drb1_high\", \"hla_match_dqb1_high\"]].sum(axis=1), inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "  def fillna_and_cast(self, df):\n",
    "    RMV = [\"ID\",\"efs\",\"efs_time\"]\n",
    "    FEATURES = [c for c in df.columns if not c in RMV]\n",
    "\n",
    "    for c in FEATURES:\n",
    "\n",
    "      if df[c].dtype == \"object\":\n",
    "        df[c] = df[c].fillna(\"unknown\")\n",
    "        df[c] = df[c].astype(\"category\")\n",
    "\n",
    "      else:\n",
    "        df[c] = df[c].fillna(-1)\n",
    "\n",
    "        if df[c].dtype == \"int64\":\n",
    "          df[c] = df[c].astype(\"int32\")\n",
    "        else:\n",
    "          df[c] = df[c].astype(\"float32\")\n",
    "\n",
    "    return df\n",
    "\n",
    "  def info(self, df):\n",
    "\n",
    "    print(f'\\nShape of dataframe: {df.shape}')\n",
    "\n",
    "    mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage: {:.2f} MB\\n'.format(mem))\n",
    "\n",
    "    display(df.head())\n",
    "\n",
    "  def apply_fe(self, df):\n",
    "   self.check_df(df)\n",
    "   df = self.fill_hla(df)\n",
    "   df = self.fillna_and_cast(df)\n",
    "   self.info(df)\n",
    "\n",
    "   return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:37.628103Z",
     "iopub.status.busy": "2025-03-05T10:14:37.627792Z",
     "iopub.status.idle": "2025-03-05T10:14:37.955063Z",
     "shell.execute_reply": "2025-03-05T10:14:37.954140Z",
     "shell.execute_reply.started": "2025-03-05T10:14:37.628080Z"
    },
    "id": "05jmDHHT1qjD",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5f0210b8-7628-4a64-f670-ac0c37d3b124",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fe = FeatureEngineering()\n",
    "train = fe.apply_fe(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:37.956352Z",
     "iopub.status.busy": "2025-03-05T10:14:37.956041Z",
     "iopub.status.idle": "2025-03-05T10:14:38.037812Z",
     "shell.execute_reply": "2025-03-05T10:14:38.036974Z",
     "shell.execute_reply.started": "2025-03-05T10:14:37.956321Z"
    },
    "id": "fZKKZ3yE1s1H",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "392fa69e-49e9-4b2e-bdb8-860db0abe07b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = fe.apply_fe(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:38.038909Z",
     "iopub.status.busy": "2025-03-05T10:14:38.038662Z",
     "iopub.status.idle": "2025-03-05T10:14:38.051708Z",
     "shell.execute_reply": "2025-03-05T10:14:38.050772Z",
     "shell.execute_reply.started": "2025-03-05T10:14:38.038889Z"
    },
    "id": "3Xv848D61uWr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EDA:\n",
    "  def __init__(self, train):\n",
    "    self.train = train\n",
    "\n",
    "  def plot_distribution(self, feature, figsize, color=False):\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if color:\n",
    "      sns.histplot(x=feature, data=self.train, hue=feature)\n",
    "    else:\n",
    "      sns.histplot(x=feature, data=self.train)\n",
    "\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "\n",
    "    plt.xlabel(f\"{feature}\")\n",
    "    plt.ylabel(\"Count\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "  def plot_heatmap(self, columns, palette, data_name, mask=True):\n",
    "    corr_matrix = self.train[columns].corr()\n",
    "\n",
    "    if mask:\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "    plt.figure(figsize=(14, 12))\n",
    "\n",
    "    sns.heatmap(corr_matrix, cmap=palette, mask=mask, annot=True, fmt=\".2f\",\n",
    "               square=True, cbar_kws={\"shrink\": .8}, linewidths=.5)\n",
    "    plt.title(f\"Heatmap for {data_name} data\", fontsize=16, color=(0.0, 0.0, 0.0))\n",
    "    plt.show()\n",
    "\n",
    "  def plot_target(self, target):\n",
    "    plt.figure(figsize=(6, 3))\n",
    "\n",
    "    sns.histplot(data=train, x=target, hue='efs', element='step', stat='density', common_norm=False, palette=\"coolwarm\")\n",
    "    plt.legend(title='efs')\n",
    "    plt.title(f'Distribution of {target} by EFS')\n",
    "    plt.xlabel(target)\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "\n",
    "  def _plot_cv(self, scores, title, metric='Stratified C-Index'):\n",
    "\n",
    "    fold_scores = [round(score, 3) for score in scores]\n",
    "    mean_score = round(np.mean(scores), 3)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x = list(range(1, len(fold_scores) + 1)),\n",
    "        y = fold_scores,\n",
    "        mode = 'markers',\n",
    "        name = 'Fold Scores',\n",
    "        marker = dict(size = 27, symbol='diamond'),\n",
    "        text = [f'{score:.3f}' for score in fold_scores],\n",
    "        hovertemplate = 'Fold %{x}: %{text}<extra></extra>',\n",
    "        hoverlabel = dict(font=dict(size=18))\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x = [1, len(fold_scores)],\n",
    "        y = [mean_score, mean_score],\n",
    "        mode = 'lines',\n",
    "        name = f'Mean: {mean_score:.3f}',\n",
    "        line = dict(dash = 'dash', color = '#B22222'),\n",
    "        hoverinfo = 'none'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title = f'{title} | Cross-validation Mean {metric} Score: {mean_score}',\n",
    "        xaxis_title = 'Fold',\n",
    "        yaxis_title = f'{metric} Score',\n",
    "        plot_bgcolor = 'rgba(247, 230, 202, 1)',\n",
    "        paper_bgcolor = 'rgba(247, 230, 202, 1)',\n",
    "        xaxis = dict(\n",
    "            gridcolor = 'grey',\n",
    "            tickmode = 'linear',\n",
    "            tick0 = 1,\n",
    "            dtick = 1,\n",
    "            range = [0.5, len(fold_scores) + 0.5],\n",
    "            zerolinecolor = 'grey'\n",
    "        ),\n",
    "        yaxis = dict(\n",
    "            gridcolor = 'grey',\n",
    "            zerolinecolor = 'grey'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "  def plot_importance(self, model, model_name, target, features):\n",
    "    importance_df = pd.DataFrame(\n",
    "        {\n",
    "          'Importance': model.feature_importances_,\n",
    "          'Feature': features.columns\n",
    "        }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\n",
    "    plt.title(f\"Feature Importance of {model_name}-{target}\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:38.052874Z",
     "iopub.status.busy": "2025-03-05T10:14:38.052556Z",
     "iopub.status.idle": "2025-03-05T10:14:39.079314Z",
     "shell.execute_reply": "2025-03-05T10:14:39.078387Z",
     "shell.execute_reply.started": "2025-03-05T10:14:38.052845Z"
    },
    "id": "JEl5BEE-1v3v",
    "outputId": "2e51b34a-8887-4682-9e0d-2f6d4d77d010",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eda = EDA(train)\n",
    "\n",
    "num_cols = [col for col in train.columns if train[col].dtype != \"category\"]\n",
    "eda.plot_heatmap(num_cols, \"YlGnBu\", \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:39.080679Z",
     "iopub.status.busy": "2025-03-05T10:14:39.080341Z",
     "iopub.status.idle": "2025-03-05T10:14:39.325871Z",
     "shell.execute_reply": "2025-03-05T10:14:39.325006Z",
     "shell.execute_reply.started": "2025-03-05T10:14:39.080643Z"
    },
    "id": "f4YL7rje1yQj",
    "outputId": "aefa3c7f-1e8a-4810-d662-13f867f22dc2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eda.plot_distribution(\"age_at_hct\", (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:39.327180Z",
     "iopub.status.busy": "2025-03-05T10:14:39.326836Z",
     "iopub.status.idle": "2025-03-05T10:14:39.665681Z",
     "shell.execute_reply": "2025-03-05T10:14:39.664486Z",
     "shell.execute_reply.started": "2025-03-05T10:14:39.327131Z"
    },
    "id": "7m00RneV10EE",
    "outputId": "b6f9b2c4-38cf-479b-d9a8-07e2e48c0984",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eda.plot_distribution(\"race_group\", (20,6), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:39.667318Z",
     "iopub.status.busy": "2025-03-05T10:14:39.666956Z",
     "iopub.status.idle": "2025-03-05T10:14:40.012437Z",
     "shell.execute_reply": "2025-03-05T10:14:40.011515Z",
     "shell.execute_reply.started": "2025-03-05T10:14:39.667284Z"
    },
    "id": "vSYbR7pE11lD",
    "outputId": "6a48becc-9718-4ddb-9b6e-ac83257b5bbe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.hist(train.efs_time[train.efs == 0], bins=np.linspace(0, 160, 41), label='efs=0: patient still lives at this time', alpha=0.5)\n",
    "plt.hist(train.efs_time[train.efs == 1], bins=np.linspace(0, 160, 41), label='efs=1: patient dies at this time', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('efs_time')\n",
    "plt.ylabel('count')\n",
    "plt.title('Target histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:40.013535Z",
     "iopub.status.busy": "2025-03-05T10:14:40.013265Z",
     "iopub.status.idle": "2025-03-05T10:14:40.020140Z",
     "shell.execute_reply": "2025-03-05T10:14:40.019219Z",
     "shell.execute_reply.started": "2025-03-05T10:14:40.013512Z"
    },
    "id": "p2G9Ofqo13ar",
    "outputId": "0830b910-92d5-4a62-9161-1d73c6baff2e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cat_cols = [col for col in train.columns if train[col].dtype == \"category\"]\n",
    "num_cols = [col for col in train.columns if train[col].dtype != \"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:40.021397Z",
     "iopub.status.busy": "2025-03-05T10:14:40.021109Z",
     "iopub.status.idle": "2025-03-05T10:14:40.039565Z",
     "shell.execute_reply": "2025-03-05T10:14:40.038774Z",
     "shell.execute_reply.started": "2025-03-05T10:14:40.021369Z"
    },
    "id": "ID0h38U518bH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformTarget:\n",
    "\n",
    "  def __init__(self, data, cat_cols, penalizer, n_splits):\n",
    "    self.data = train\n",
    "    self.cat_cols = cat_cols\n",
    "\n",
    "    self._length = len(self.data)\n",
    "    self._penalizer = penalizer\n",
    "    self._n_splits = n_splits\n",
    "\n",
    "  def _prepare_cv(self):\n",
    "\n",
    "    oof_preds = np.zeros(self._length)\n",
    "\n",
    "    cv = KFold(n_splits=self._n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    return cv, oof_preds\n",
    "\n",
    "  def validate_model(self, preds, title):\n",
    "\n",
    "    y_true = self.data[['ID', 'efs', 'efs_time', 'race_group']].copy()\n",
    "    y_pred = self.data[['ID']].copy()\n",
    "\n",
    "    y_pred['prediction'] = preds\n",
    "\n",
    "    c_index_score = score(y_true.copy(), y_pred.copy(), 'ID')\n",
    "    print(f'Overall Stratified C-Index Score for {title}: {c_index_score:.4f}')\n",
    "\n",
    "  def cox_target(self):\n",
    "\n",
    "    cv, oof_preds = self._prepare_cv()\n",
    "\n",
    "    # Apply one hot encoding to categorical columns\n",
    "    data = pd.get_dummies(self.data, columns=self.cat_cols, drop_first=True).drop('ID', axis=1)\n",
    "\n",
    "    for train_index, valid_index in cv.split(data):\n",
    "\n",
    "      train_data = data.iloc[train_index]\n",
    "      valid_data = data.iloc[valid_index]\n",
    "\n",
    "      # Drop constant columns if they exist\n",
    "      train_data = train_data.loc[:, train_data.nunique() > 1]\n",
    "      valid_data = valid_data[train_data.columns]\n",
    "\n",
    "      cph = CoxPHFitter(penalizer=self._penalizer)\n",
    "      cph.fit(train_data, duration_col='efs_time', event_col='efs')\n",
    "\n",
    "      oof_preds[valid_index] = cph.predict_partial_hazard(valid_data)\n",
    "\n",
    "    self.data['cox_target'] = oof_preds\n",
    "    self.validate_model(oof_preds, 'Cox')\n",
    "\n",
    "    return self.data\n",
    "\n",
    "  def kmf_target(self):\n",
    "\n",
    "    cv, oof_preds = self._prepare_cv()\n",
    "\n",
    "    for train_index, valid_index in cv.split(self.data):\n",
    "\n",
    "      train_data = self.data.iloc[train_index]\n",
    "      valid_data = self.data.iloc[valid_index]\n",
    "\n",
    "      kmf = KaplanMeierFitter()\n",
    "      kmf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n",
    "\n",
    "      oof_preds[valid_index] = kmf.survival_function_at_times(valid_data['efs_time']).values\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    kmf.plot(at_risk_counts=True)\n",
    "    plt.plot()\n",
    "\n",
    "    self.data['kmf_target'] = oof_preds\n",
    "    self.validate_model(oof_preds, 'Kaplan-Meier')\n",
    "\n",
    "    return self.data\n",
    "\n",
    "  def naf_target(self):\n",
    "\n",
    "    cv, oof_preds = self._prepare_cv()\n",
    "\n",
    "    for train_index, valid_index in cv.split(self.data):\n",
    "\n",
    "      train_data = self.data.iloc[train_index]\n",
    "      valid_data = self.data.iloc[valid_index]\n",
    "\n",
    "      naf = NelsonAalenFitter()\n",
    "      naf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n",
    "\n",
    "      oof_preds[valid_index] = -naf.cumulative_hazard_at_times(valid_data['efs_time']).values\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    naf.plot(at_risk_counts=True)\n",
    "    plt.plot()\n",
    "\n",
    "    self.data['naf_target'] = oof_preds\n",
    "    self.validate_model(oof_preds, 'Nelson-Aalen')\n",
    "\n",
    "    return self.data\n",
    "\n",
    "  def coxloss_target(self):\n",
    "\n",
    "    self.data['cox_loss'] = self.data.efs_time.copy()\n",
    "    self.data.loc[self.data.efs == 0, 'cox_loss'] *= -1\n",
    "\n",
    "    return self.data\n",
    "\n",
    "  def transform_separate(self):\n",
    "    \"\"\"Transform the target by separating events from non-events\n",
    "\n",
    "    From https://www.kaggle.com/code/mtinti/cibmtr-lofo-feature-importance-gpu-accelerated\"\"\"\n",
    "    cv, oof_preds = self._prepare_cv()\n",
    "    self.data[\"seperate_target\"] = np.nan\n",
    "    for train_index, valid_index in cv.split(self.data):\n",
    "\n",
    "      train_data = self.data.iloc[train_index]\n",
    "      valid_data = self.data.iloc[valid_index]\n",
    "\n",
    "      transformed = train_data['efs_time'].values.copy()\n",
    "      event = train_data['efs'].values\n",
    "\n",
    "      mx = transformed[event == 1].max() # last patient who dies\n",
    "      mn = transformed[event == 0].min() # first patient who survives\n",
    "      transformed[event == 0] = train_data['efs_time'][event == 0] + mx - mn\n",
    "      transformed = rankdata(transformed)\n",
    "      transformed[event == 0] += len(transformed) // 2\n",
    "      transformed = transformed / transformed.max()\n",
    "\n",
    "      self.data.loc[train_index, 'seperate_target'] = -transformed\n",
    "\n",
    "    return self.data\n",
    "\n",
    "  def transform_rank_log(self):\n",
    "      \"\"\"Transform the target by stretching the range of eventful efs_times and compressing the range of event_free efs_times\n",
    "\n",
    "      From https://www.kaggle.com/code/cdeotte/nn-mlp-baseline-cv-670-lb-676\"\"\"\n",
    "\n",
    "      cv, oof_preds = self._prepare_cv()\n",
    "\n",
    "      self.data['rank_log_target'] = np.nan\n",
    "\n",
    "      for train_index, valid_index in cv.split(self.data):\n",
    "\n",
    "        train_data = self.data.iloc[train_index]\n",
    "        valid_data = self.data.iloc[valid_index]\n",
    "\n",
    "        transformed = train_data['efs_time'].values.copy()\n",
    "        event = train_data['efs'].values\n",
    "\n",
    "        mx = transformed[event == 1].max() # last patient who dies\n",
    "        mn = transformed[event == 0].min() # first patient who survives\n",
    "        transformed[event == 0] = train_data['efs_time'][event == 0] + mx - mn\n",
    "        transformed = rankdata(transformed)\n",
    "        transformed[event == 0] += len(transformed) * 2\n",
    "        transformed = transformed / transformed.max()\n",
    "        transformed = np.log(transformed)\n",
    "\n",
    "        self.data.loc[train_index, 'rank_log_target'] = -transformed\n",
    "\n",
    "      return self.data\n",
    "\n",
    "  def transform_quantile(self):\n",
    "      \"\"\"Transform the target by stretching the range of eventful efs_times and compressing the range of event_free efs_times\n",
    "\n",
    "      From https://www.kaggle.com/code/ambrosm/esp-eda-which-makes-sense\"\"\"\n",
    "\n",
    "      cv, oof_preds = self._prepare_cv()\n",
    "\n",
    "      self.data['quantile_target'] = np.nan\n",
    "\n",
    "      for train_index, valid_index in cv.split(self.data):\n",
    "\n",
    "        train_data = self.data.iloc[train_index]\n",
    "        valid_data = self.data.iloc[valid_index]\n",
    "\n",
    "        transformed = np.full(len(train_data['efs_time']), np.nan)\n",
    "        event = train_data['efs'].values\n",
    "\n",
    "        transformed_dead = quantile_transform(- train_data['efs_time'][event == 1].values.reshape(-1, 1)).ravel()\n",
    "        transformed[event == 1] = transformed_dead\n",
    "        transformed[event == 0] = transformed_dead.min() - 0.3\n",
    "\n",
    "        self.data.loc[train_index, 'quantile_target'] = -transformed\n",
    "\n",
    "      return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T10:14:40.040501Z",
     "iopub.status.busy": "2025-03-05T10:14:40.040286Z",
     "iopub.status.idle": "2025-03-05T10:14:40.055303Z",
     "shell.execute_reply": "2025-03-05T10:14:40.054509Z",
     "shell.execute_reply.started": "2025-03-05T10:14:40.040482Z"
    },
    "id": "Wqzhuy-S19Cr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Modelling:\n",
    "  def __init__(self, train, cat_cols, num_cols, early_stop):\n",
    "    self.train = train\n",
    "    self.num_cols = num_cols\n",
    "    self.cat_cols = cat_cols\n",
    "    self.early_stop = early_stop\n",
    "    self.results = []\n",
    "\n",
    "    self.targets = TransformTarget(self.train, self.cat_cols, CFG.penalizer, CFG.n_splits)\n",
    "    self.eda = EDA(self.train)\n",
    "\n",
    "    self.cv, self.oof_preds = self.targets._prepare_cv()\n",
    "\n",
    "  def create_targets(self):\n",
    "    self.train = self.targets.cox_target()\n",
    "    self.train = self.targets.kmf_target()\n",
    "    self.train = self.targets.naf_target()\n",
    "    self.train = self.targets.coxloss_target()\n",
    "    self.train = self.targets.transform_separate()\n",
    "    self.train = self.targets.transform_rank_log()\n",
    "    self.train = self.targets.transform_quantile()\n",
    "\n",
    "  def train_model(self, target, model_name, params):\n",
    "    print(f\"\\n----- Training {model_name} with {target} -----\")\n",
    "\n",
    "    for col in self.cat_cols:\n",
    "      self.train[col] = self.train[col].astype('category')\n",
    "\n",
    "    X = self.train.drop([\"ID\", \"efs_time\", \"efs\", \"cox_target\",\n",
    "                         \"kmf_target\", \"naf_target\", \"cox_loss\",\n",
    "                         \"seperate_target\", \"rank_log_target\", \"quantile_target\"], axis=1)\n",
    "    y = self.train[target]\n",
    "\n",
    "    self.models, fold_scores = [], []\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(self.cv.split(X, y)):\n",
    "\n",
    "      print(f\"\\n################ Fold {fold+1} ################\")\n",
    "\n",
    "      X_train = X.iloc[train_index]\n",
    "      X_valid = X.iloc[valid_index]\n",
    "\n",
    "      y_train = y.iloc[train_index]\n",
    "      y_valid = y.iloc[valid_index]\n",
    "\n",
    "      if model_name.startswith(\"CatBoost\"):\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            **params,\n",
    "            cat_features=self.cat_cols,\n",
    "            verbose=False,\n",
    "            task_type=\"CPU\"\n",
    "        )\n",
    "\n",
    "      if model_name.startswith(\"XGBoost\"):\n",
    "\n",
    "        model = XGBRegressor(\n",
    "            **params,\n",
    "            verbose=500,\n",
    "            device=\"cuda\"\n",
    "        )\n",
    "\n",
    "      if model_name.startswith(\"LightGBM\"):\n",
    "\n",
    "        model = LGBMRegressor(**params, device=\"gpu\",)\n",
    "\n",
    "      model.fit(\n",
    "          X_train, y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "      )\n",
    "\n",
    "      self.models.append(model)\n",
    "\n",
    "      self.oof_preds[valid_index] = model.predict(X_valid)\n",
    "\n",
    "      y_true_fold = self.train.iloc[valid_index][['ID', 'efs', 'efs_time', 'race_group']].copy()\n",
    "      y_pred_fold = self.train.iloc[valid_index][['ID']].copy()\n",
    "\n",
    "      y_pred_fold['prediction'] = self.oof_preds[valid_index]\n",
    "\n",
    "      fold_score = score(y_true_fold, y_pred_fold, 'ID')\n",
    "\n",
    "      print(f\"    C-index: {fold_score}\")\n",
    "      fold_scores.append(fold_score)\n",
    "\n",
    "    self.eda._plot_cv(fold_scores, model_name)\n",
    "    self.eda.plot_importance(model, model_name, target, X)\n",
    "\n",
    "    self.targets.validate_model(self.oof_preds, model_name)\n",
    "\n",
    "    avg_score = np.mean(fold_scores)\n",
    "    self.results.append((f\"{target}_{model_name}\", avg_score))\n",
    "    print(f\"Overall CV Score for {model_name} with {target}: {avg_score}\")\n",
    "\n",
    "    return self.models, self.oof_preds\n",
    "\n",
    "  def infer_model(self, data, models):\n",
    "\n",
    "    data = data.drop(['ID'], axis=1)\n",
    "\n",
    "    return np.mean([model.predict(data) for model in models], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.173Z",
     "iopub.execute_input": "2025-03-05T10:14:40.056604Z",
     "iopub.status.busy": "2025-03-05T10:14:40.056305Z"
    },
    "id": "eFTpl4H62ALf",
    "outputId": "8a97d048-5d61-49da-8928-7846028b73fd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "m = Modelling(train, cat_cols, num_cols, CFG.early_stop)\n",
    "m.create_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.173Z"
    },
    "id": "STwtW-AL2A4D",
    "outputId": "21ce9c48-f493-4dcc-a79d-6eb819b2f10c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m.eda.plot_distribution(\"cox_target\", (6,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.173Z"
    },
    "id": "T9W1G35D2DR3",
    "outputId": "435d00b4-55d5-4b3f-b618-3031be9cd06c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m.eda.plot_distribution(\"kmf_target\", (6,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.173Z"
    },
    "id": "VuYR8RGA2E2t",
    "outputId": "078662cf-f6bf-44c8-8862-9ad05aeaeea1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m.eda.plot_distribution(\"naf_target\", (6,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.173Z"
    },
    "id": "7Gexkc7d2HaP",
    "outputId": "bca884c3-8649-466e-8991-634d39a26d64",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m.eda.plot_target(\"cox_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.173Z"
    },
    "id": "JexbT7Vn_D2_",
    "outputId": "7503bc16-162e-46c4-e0cd-6f8fff061198",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m.eda.plot_target(\"kmf_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.173Z"
    },
    "id": "4f9VL6ZY_HRc",
    "outputId": "bf49b82e-7265-4e07-d323-2fb589463259",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m.eda.plot_target(\"naf_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.173Z"
    },
    "id": "C7Nvt9962H5P",
    "outputId": "c3dd49fe-1cb4-4965-e4ea-b47c56788d1e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m.eda.plot_target(\"seperate_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.173Z"
    },
    "id": "u9dC_fTT2JZm",
    "outputId": "847b1b11-98ea-4bd0-f3d1-82ae981177fc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m.eda.plot_target(\"rank_log_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "2q2yeitb2M-j",
    "outputId": "32f92a83-6b0c-4b2f-a7ef-61cb349ab820",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m.eda.plot_target(\"quantile_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "2tRb-sB52OkX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# 0.65\n",
    "# cat_cox, cat_cox_oof = m.train_model(\"cox_target\", \"CatBoost\", CFG.cat_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "nnitTqtx2RGv",
    "outputId": "d05c01fb-85aa-4aa2-d447-c7332f61fbaf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 0.66\n",
    "xgb_cox, xgb_cox_oof = m.train_model(\"cox_target\", \"XGBoost\", CFG.xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "NI7JvSaMt5-O",
    "outputId": "a9100dba-93df-44db-92e9-57fd7d99f230",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#0.65\n",
    "lgb_cox, lgb_cox_oof = m.train_model(\"cox_target\", \"LightGBM\", CFG.lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "wq4CvpvxuBnP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# cat_kmf, cat_kmf_oof = m.train_model(\"kmf_target\", \"CatBoost\", CFG.cat_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "HsF1VKXcuFv2",
    "outputId": "3d347dec-dde8-473c-8fc0-b05a1118008e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#0.67\n",
    "xgb_kmf, xgb_kmf_oof = m.train_model(\"kmf_target\", \"XGBoost\", CFG.xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "WBuuXGm5vyyn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "#0.64\n",
    "# lgb_kmf, lgb_kmf_oof = m.train_model(\"kmf_target\", \"LightGBM\", CFG.lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "wgNTIGoyv3db",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# cat_naf, cat_naf_oof = m.train_model(\"naf_target\", \"CatBoost\", CFG.cat_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "3h_0HhfdwHSK",
    "outputId": "3808ffed-c34c-4c9c-e90c-2fb3f1efc655",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#0.67\n",
    "xgb_naf, xgb_naf_oof = m.train_model(\"naf_target\", \"XGBoost\", CFG.xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "sITSU1FTwJFR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "#0.64\n",
    "# lgb_naf, lgb_naf_oof = m.train_model(\"naf_target\", \"LightGBM\", CFG.lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "6pQ3P0WI_Uub",
    "outputId": "909b8325-bd90-4a0b-d069-7a931ea772b6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#0.66\n",
    "#cat_cox_losses, cat_cox_losses_oof = m.train_model(\"cox_loss\", \"CatBoost\", CFG.cat_cox_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.174Z"
    },
    "id": "ePZxZ6is_flV",
    "outputId": "3c784707-ddd0-4fb3-d617-e5b2b11cb7fb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#0.67\n",
    "xgb_cox_losses, xgb_cox_losses_oof = m.train_model(\"cox_loss\", \"XGBoost\", CFG.xgb_cox_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "VgijJHzQ_s-b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# cat_seperate_target, cat_seperate_target_oof = m.train_model(\"seperate_target\", \"CatBoost\", CFG.cat_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "gnG-eUQWLNft",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "#0.63\n",
    "# lgb_seperate_target, lgb_seperate_target_oof = m.train_model(\"seperate_target\", \"LightGBM\", CFG.lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "47fdI2_HMcGg",
    "outputId": "fe4da22a-c21d-446f-9a87-4ea81bd9d67c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#0.66\n",
    "xgb_seperate_target, xgb_seperate_target_oof = m.train_model(\"seperate_target\", \"XGBoost\", CFG.xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "iHUNBX2YMhhP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 0.64\n",
    "#lgb_rank_log_target, lgb_rank_log_target_oof = m.train_model(\"rank_log_target\", \"LightGBM\", CFG.lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "CgHH3Ixh_2z8",
    "outputId": "1f1834cf-7bd5-44fe-af40-a6eeaa4b06cd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#0.67\n",
    "xgb_rank_log_target, xgb_rank_log_target_oof = m.train_model(\"rank_log_target\", \"XGBoost\", CFG.xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "9ziLn3UR_86j",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "#0.32\n",
    "# lgb_quantile_target, lgb_quantile_target_oof = m.train_model(\"quantile_target\", \"LightGBM\", CFG.lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "OHbYg63QMkZu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "# 0.30\n",
    "#xgb_quantile_target, xgb_quantile_target_oof = m.train_model(\"quantile_target\", \"XGBoost\", CFG.xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "V7dOMJIgpl5u",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# cat_cox_preds = m.infer_model(test, cat_cox)\n",
    "xgb_cox_preds = m.infer_model(test, xgb_cox)\n",
    "lgb_cox_preds = m.infer_model(test, lgb_cox)\n",
    "\n",
    "# cat_kmf_preds = m.infer_model(test, cat_kmf)\n",
    "xgb_kmf_preds = m.infer_model(test, xgb_kmf)\n",
    "\n",
    "# cat_naf_preds = m.infer_model(test, cat_naf)\n",
    "xgb_naf_preds = m.infer_model(test, xgb_naf)\n",
    "\n",
    "#cat_cox_losses_preds = m.infer_model(test, cat_cox_losses)\n",
    "xgb_cox_losses_preds = m.infer_model(test, xgb_cox_losses)\n",
    "\n",
    "# cat_seperate_target_preds = m.infer_model(test, cat_seperate_target)\n",
    "xgb_seperate_target_preds = m.infer_model(test, xgb_seperate_target)\n",
    "\n",
    "xgb_rank_log_target_preds = m.infer_model(test, xgb_rank_log_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "dE_5eaXOp04l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "oof_preds = [xgb_cox_oof, lgb_cox_oof,\n",
    "             xgb_kmf_oof,\n",
    "             xgb_naf_oof,\n",
    "             xgb_cox_losses_oof,\n",
    "             xgb_seperate_target_oof,\n",
    "             xgb_rank_log_target_oof]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "DP2DOWPup86G",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preds = [xgb_cox_preds, lgb_cox_preds,\n",
    "         xgb_kmf_preds,\n",
    "         xgb_naf_preds,\n",
    "         xgb_cox_losses_preds,\n",
    "         xgb_seperate_target_preds,\n",
    "         xgb_rank_log_target_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "yRZghVrkqEl5",
    "outputId": "e826f4e1-3410-405c-8064-357364bad46c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "y_pred = train[[\"ID\"]].copy()\n",
    "\n",
    "ranked_oof_preds = np.array([rankdata(p) for p in oof_preds])\n",
    "\n",
    "ensemble_oof_preds = np.dot(CFG.weights, ranked_oof_preds)\n",
    "\n",
    "m.targets.validate_model(ensemble_oof_preds, 'Ensemble Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "q8kxIyboqUTn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ranked_preds = np.array([rankdata(p) for p in preds])\n",
    "ensemble_preds = np.sum(ranked_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-05T10:15:40.175Z"
    },
    "id": "UBNfi4F52YfX",
    "outputId": "5a22b5c1-ce6c-4a97-a26c-d7e4aabc9ee1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create submission\n",
    "subm_data = pd.read_csv(CFG.subm_path)\n",
    "subm_data['prediction'] = ensemble_preds\n",
    "subm_data.to_csv('submission3.csv', index=False)\n",
    "display(subm_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load submission files\n",
    "sub1 = pd.read_csv('/kaggle/working/submission1.csv')\n",
    "sub2 = pd.read_csv('/kaggle/working/submission2.csv')\n",
    "sub3 = pd.read_csv('/kaggle/working/submission3.csv')\n",
    "\n",
    "# Calculate ranks for each submission's predictions\n",
    "rank1 = rankdata(sub1['prediction'], method='average')\n",
    "rank2 = rankdata(sub2['prediction'], method='average') \n",
    "rank3 = rankdata(sub3['prediction'], method='average')\n",
    "\n",
    "# Create DataFrame of ranks and average them\n",
    "rank_df = pd.DataFrame({\n",
    "    'rank1': rank1,\n",
    "    'rank2': rank2,\n",
    "    'rank3': rank3\n",
    "})\n",
    "ensemble_rank = rank_df.mean(axis=1)\n",
    "\n",
    "# Create final submission file with averaged ranks\n",
    "final_sub = sub1[['ID']].copy()\n",
    "final_sub['prediction'] = ensemble_rank\n",
    "final_sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    },
    {
     "sourceId": 211322530,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219607918,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 225897125,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
